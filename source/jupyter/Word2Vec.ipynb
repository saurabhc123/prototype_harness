{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gala bingo club buy large high street bingo operator gala take\n",
      "Loading word2vec model from file:/Users/saur6410/Google Drive/VT/Old/Thesis/Source/ThesisPython/data/GoogleNews-vectors-negative300.bin\n",
      "[('hurricanes', 0.5880494117736816), ('Hurricane_Ike', 0.5491310358047485), ('Hurricane_Rita', 0.5269111394882202), ('Hurricane_Gustav', 0.5230242013931274), ('Hurricane_Katrina', 0.522605299949646), ('Hurricane_Wilma', 0.5225527286529541), ('Hurricane_Ivan', 0.5218238830566406), ('Hurricane_Frances', 0.5146640539169312), ('Hurricane_Charley', 0.5122226476669312), ('storm', 0.4903242886066437)]\n",
      "[  3.66145410e-02  -2.80793943e-02  -9.30207297e-02   2.64713243e-02\n",
      "   1.62044093e-02   4.40364070e-02  -4.25520353e-02  -7.17447102e-02\n",
      "   1.41510248e-01  -4.60774219e-03   4.79947366e-02  -5.78905605e-02\n",
      "   4.45312001e-02  -2.00390406e-02   7.17447102e-02   6.87759668e-02\n",
      "   2.16471106e-02   6.40136003e-03  -4.51496895e-03  -8.75780284e-02\n",
      "   8.41144845e-02  -5.22004627e-02   6.97655454e-02   3.43879834e-02\n",
      "   1.18255071e-01  -3.57177318e-03   4.47785966e-02  -8.80728140e-02\n",
      "   1.20605333e-02   7.02603385e-02  -3.73567268e-02   3.85937057e-02\n",
      "  -6.97655454e-02  -1.10091018e-02  -5.64371119e-04  -8.06509480e-02\n",
      "  -6.48176372e-02  -7.66926184e-02  -5.59113957e-02   6.33332580e-02\n",
      "   2.11523194e-02   8.41144845e-02   1.46458164e-01   9.30207297e-02\n",
      "  -4.67577577e-02  -1.28645688e-01  -3.72639555e-03   4.77473401e-02\n",
      "  -1.85546670e-02   7.66926184e-02   3.36457938e-02  -5.24478555e-02\n",
      "  -1.66992005e-02   6.23436794e-02   2.75846049e-02  -4.23664879e-03\n",
      "  -2.07812265e-02  -2.28840876e-02   6.38280511e-02   4.67577577e-02\n",
      "  -7.32290819e-02   1.54003734e-02   1.70702934e-02  -1.53385242e-02\n",
      "   6.18488863e-02   4.23046388e-02  -4.20572422e-02   2.17708088e-02\n",
      "  -1.79361776e-02  -1.71939917e-02  -5.29426485e-02  -3.68619375e-02\n",
      "   3.49446223e-03   2.53580436e-02  -1.19244657e-01   1.14420438e-02\n",
      "   6.77863806e-02  -6.18488863e-02  -1.05885297e-01  -1.51406080e-01\n",
      "  -5.93749322e-02   6.11067005e-02  -5.44270203e-02  -9.30207297e-02\n",
      "  -8.11457410e-02   6.53124228e-02  -6.18488863e-02   3.93358916e-02\n",
      "  -3.56249586e-02   2.12760177e-02  -4.70051542e-02  -2.78319996e-02\n",
      "   3.15429345e-02  -6.15396444e-03  -4.72525507e-02   6.18488863e-02\n",
      "   1.02916546e-01  -2.74609067e-02  -5.86327463e-02   8.06509480e-02\n",
      "   4.25520353e-02   3.03059556e-02   4.55207825e-02   7.57648889e-03\n",
      "  -3.90884988e-02  -2.79556978e-02  -2.68424172e-02   2.73372084e-02\n",
      "  -4.74999472e-02  -1.59570128e-02  -1.19244657e-01   1.00195203e-02\n",
      "   2.70898137e-02   1.68228969e-02  -3.78515199e-02   3.63671444e-02\n",
      "   1.06380090e-01   2.28840876e-02   2.01627370e-02   3.24088186e-02\n",
      "  -8.65884423e-02  -3.95832863e-03   6.92707524e-02   7.96613693e-02\n",
      "   4.79947366e-02   2.85741854e-02   1.37304533e-02  -1.56477690e-02\n",
      "   1.27408709e-02   1.32604018e-01  -1.66218891e-03  -9.64842644e-03\n",
      "  -4.27994318e-02   6.86522666e-03   3.56249586e-02  -3.36457938e-02\n",
      "  -3.31510045e-02  -3.21614221e-02   8.59699585e-03   7.32909329e-03\n",
      "   3.15429345e-02  -2.99348626e-02  -6.87759668e-02  -3.73567268e-02\n",
      "   6.92707533e-03   1.73176883e-03   5.88801429e-02  -8.21353197e-02\n",
      "  -8.85676071e-02  -5.64061850e-02  -5.39322309e-02  -9.49998945e-02\n",
      "  -2.80793943e-02   4.87369224e-02  -8.04035552e-03  -2.89452802e-02\n",
      "   4.94791120e-02  -9.15363524e-03   6.87759668e-02   1.63281057e-02\n",
      "   6.97655454e-02   2.32551824e-02   6.98892446e-03  -2.48632524e-02\n",
      "   5.44270221e-03   1.34830577e-02   3.33984010e-02   4.67577577e-02\n",
      "   1.29635274e-01   2.27603912e-02  -4.52733859e-02  -7.66926212e-03\n",
      "   7.71874115e-02  -1.26171738e-01  -1.85546669e-04  -6.10757794e-04\n",
      "   4.62629683e-02   1.59570128e-02  -4.40364070e-02   6.72915876e-02\n",
      "  -8.70832354e-02   8.84439051e-03  -2.96874661e-02  -4.25520353e-02\n",
      "  -7.05077313e-03  -8.65884423e-02  -4.72525507e-02  -5.88801429e-02\n",
      "   8.36196989e-02   8.26301128e-02   1.11946492e-02   1.00442596e-01\n",
      "   2.83267908e-02  -5.44270203e-02  -2.99348626e-02   1.12812370e-01\n",
      "   2.55126669e-03   2.91926749e-02  -1.01432176e-02  -6.95799990e-03\n",
      "  -2.82030925e-02  -3.24088186e-02   7.61978328e-02   1.26790218e-02\n",
      "  -7.91665763e-02   8.65884423e-02   6.67968020e-02  -3.73567268e-02\n",
      "   6.87759668e-02   5.49218133e-02  -5.29426485e-02  -1.13801956e-01\n",
      "   5.88801429e-02   2.41210666e-02  -4.15624529e-02   9.69790593e-02\n",
      "   3.71093340e-02  -2.30077859e-02  -3.24088186e-02   3.61197516e-02\n",
      "   1.76887810e-02   6.48176372e-02  -2.96874661e-02  -6.92707524e-02\n",
      "   5.62824868e-03  -6.03026664e-03  -3.83463106e-03   1.09843627e-01\n",
      "   1.27656102e-01  -5.73957674e-02  -5.56639992e-02   6.18488863e-02\n",
      "   8.06509480e-02  -1.39531091e-01  -8.51040706e-02   1.19368350e-02\n",
      "   1.29882665e-02  -1.17760286e-01   1.52766751e-02  -1.84309687e-02\n",
      "   1.36562347e-01  -4.74999472e-02  -3.29036079e-02  -9.74738449e-02\n",
      "  -3.41405869e-02   5.19530661e-02   1.34583175e-01   4.32942212e-02\n",
      "  -5.26952520e-02  -4.20572422e-02  -6.82811737e-02  -3.88411023e-02\n",
      "   2.36262754e-02  -2.01627370e-02  -1.59260887e-03  -2.36262754e-02\n",
      "  -1.80598758e-02   1.52302883e-03   2.01627370e-02   2.26366930e-02\n",
      "   2.73681339e-03  -1.89257599e-02  -3.21614221e-02  -2.82030925e-02\n",
      "  -2.48632524e-02   8.70832354e-02  -4.27994318e-02   2.27294653e-03\n",
      "  -7.27342889e-02  -5.07160872e-02  -4.57681753e-02   1.86041459e-01\n",
      "   2.75846049e-02   6.97655454e-02  -7.17447102e-02   2.84504890e-02\n",
      "  -7.02603385e-02  -6.98892446e-03  -9.25259367e-02   6.43228441e-02\n",
      "   2.42756889e-03  -2.86051095e-03  -1.19986841e-02   4.65103649e-02\n",
      "   1.34830577e-02  -4.18098494e-02  -3.66145410e-02   4.35416177e-02\n",
      "   6.63020089e-02  -6.09211531e-03   3.41405869e-02   1.59570128e-02\n",
      "  -4.00780812e-02  -5.24478555e-02   8.21353197e-02   1.02421761e-01\n",
      "   2.93782214e-03   6.77863806e-02   1.16275912e-02   4.40364070e-02]\n",
      "0.0509266820154\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import Placeholders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pretrained_word_vector_binary = '/Users/saur6410/Google Drive/VT/Old/Thesis/Source/ThesisPython/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class word2vec:\n",
    "    model = None\n",
    "    models = {}\n",
    "    stop = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Loading word2vec model from file:{}\".format(pretrained_word_vector_binary))\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_word_vector_binary, binary=True)\n",
    "            model.init_sims(replace=True)  # we are no longer training the model so allow it to trim memory\n",
    "            word2vec.model = model\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_model_from_file(self,name):\n",
    "        if not name in word2vec.models:\n",
    "            sentences = []\n",
    "            with open(name,\"r\") as f:\n",
    "                sentences = map(lambda x: word2vec.extract_sentence(x).split(), f.readlines())\n",
    "            file_model = gensim.models.Word2Vec(sentences=sentences, size=300, min_count=1, window=5)\n",
    "            # sentences = []\n",
    "            # sentences.append('ebola threat real allow african conference nyc risky stupid wrong'.split())\n",
    "            # file_model.similar_by_vector(sentences)\n",
    "            word2vec.models[name] = file_model.wv\n",
    "        return word2vec.models[name]\n",
    "\n",
    "\n",
    "    def extract_sentence(self, line):\n",
    "        lineContent = line.split(',')\n",
    "        return (lineContent[1] if len(lineContent) > 1 else lineContent[0])\n",
    "\n",
    "    def avg_feature_vector(self, words, model, num_features , index2word_set):\n",
    "        # function to average all words vectors in a given paragraph\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "        model = self.get_model()\n",
    "\n",
    "        # list containing names of words in the vocabulary\n",
    "        # index2word_set = set(model.index2word) this is moved as input param for performance reasons\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords += 1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "        if (nwords > 0):\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        model = self.get_model()\n",
    "        vec = model[word]\n",
    "        return vec[0:Placeholders.n_inputs]\n",
    "\n",
    "    def get_sentence_vector(self, sentence):\n",
    "        featureVec = np.zeros((1,300))\n",
    "        nwords = 0\n",
    "        clean_sentence = self.clean_sent(self.wordnet_lemmatizer, sentence)\n",
    "        model = self.get_model()\n",
    "        words = clean_sentence.rstrip().split(\" \")\n",
    "        if(len(words) == 0):\n",
    "            return featureVec\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_vector = model[word]\n",
    "                nwords += 1\n",
    "                #print word_vector\n",
    "                featureVec = np.add(featureVec, word_vector)\n",
    "            except:\n",
    "                pass #Swallow exception\n",
    "        if (nwords > 0):\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "\n",
    "    def get_sentence_matrix(self, sentence):\n",
    "        sentence_matrix = []\n",
    "        featureVec = np.zeros((1,300))\n",
    "        nwords = 0\n",
    "        clean_sentence = self.clean_sent(self.wordnet_lemmatizer, sentence)\n",
    "        model = self.get_model()\n",
    "        words = clean_sentence.rstrip().split(\" \")\n",
    "        if(len(words) == 0):\n",
    "            return sentence_matrix\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_vector = self.get_word_vector(word)\n",
    "                nwords += 1\n",
    "                #print word_vector\n",
    "                sentence_matrix.append(word_vector)\n",
    "            except:\n",
    "                pass #Swallow exception\n",
    "        padded_sentence_matrix = self.getPaddedSentenceMatrix(np.array(sentence_matrix))\n",
    "        return padded_sentence_matrix\n",
    "\n",
    "    def get_sentence_vector_ex(self, sentence):\n",
    "        try:\n",
    "            #return np.zeros((1, Placeholders.n_steps * Placeholders.n_inputs))\n",
    "            sentence_matrix = self.get_sentence_matrix(sentence)\n",
    "            return sentence_matrix.reshape((1, Placeholders.n_steps * Placeholders.n_inputs))\n",
    "        except:\n",
    "            print(\"Error with sentence:\" + sentence)\n",
    "        return np.zeros((1, Placeholders.n_steps* Placeholders.n_inputs))\n",
    "\n",
    "    def getPaddedSentenceMatrix(self, sentenceMatrix):\n",
    "        wordCount = Placeholders.n_steps\n",
    "        #print(sentenceMatrix.shape)\n",
    "        return np.vstack((sentenceMatrix,\n",
    "                        np.zeros((wordCount - np.shape(sentenceMatrix)[0], np.shape(sentenceMatrix)[1]),\n",
    "                        dtype=np.float32)))\n",
    "\n",
    "    def read_from_file(self, name, model):\n",
    "        with open(name,\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            tweets_only = map(lambda line: line.split(',')[1].rstrip().split(\" \"), lines)\n",
    "            lables_only = map(lambda line: line.split(',')[0].rstrip(), lines)\n",
    "            vecs = map(lambda t: self.avg_feature_vector(t, model, 300, model.index2word),tweets_only)\n",
    "            return zip(lables_only,vecs)\n",
    "\n",
    "    def full_pipeline(self, lem, word):\n",
    "        word = word.lower()\n",
    "        word = word.translate(string.punctuation)\n",
    "        for val in ['a', 'v', 'n']:\n",
    "            word = lem.lemmatize(word, pos=val)\n",
    "        return word\n",
    "\n",
    "\n",
    "    def clean_sent(self, lem, sent):\n",
    "        #sent = unicode(sent,errors='ignore')\n",
    "        words = sent.replace(\",\",\" \").replace(\";\", \" \").replace(\"#\",\" \").replace(\":\", \" \").replace(\"@\", \" \").split()\n",
    "        filtered_words = filter(lambda word: word.isalpha() and len(word) > 1 and word != \"http\" and word != \"rt\", [self.full_pipeline(lem, word) for word in words])\n",
    "        return ' '.join(self.filter_stopwords(filtered_words))\n",
    "\n",
    "    def filter_stopwords(self, words):\n",
    "        return filter(lambda word: word not in self.stop, words)\n",
    "\n",
    "\n",
    "# sentence = \"Gala Bingo clubs bought for 241m: The UK's largest High Street bingo operator, Gala, is being taken over by_ https://t.co/HzeeykJUd3\"\n",
    "# wv = word2vec()\n",
    "# clean_sentence = wv.clean_sent(wv.wordnet_lemmatizer,sentence)\n",
    "# print(clean_sentence)\n",
    "# sentence_matrix = wv.get_sentence_matrix(clean_sentence)\n",
    "# model = word2vec().get_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"Gala Bingo clubs bought for 241m: The UK's largest High Street bingo operator, Gala, is being taken over by_ https://t.co/HzeeykJUd3\"\n",
    "    wv = word2vec()\n",
    "    clean_sentence = wv.clean_sent(wv.wordnet_lemmatizer,sentence)\n",
    "    print(clean_sentence)\n",
    "    sentence_matrix = wv.get_sentence_matrix(clean_sentence)\n",
    "\n",
    "    model = word2vec().get_model()\n",
    "    # you can find the terms that are similar to a list of words and different from\n",
    "    # another list of words like so\n",
    "    print(model.most_similar(positive=['hurricane'], negative=['isaac']))\n",
    "\n",
    "    # you can also get the vector for a specific word by doing\n",
    "    print(model['hurricane'])\n",
    "\n",
    "    # you can ask for similarity by doing\n",
    "    print(model.similarity('hurricane', 'shooting'))\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
